{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” ClÃ©s disponibles dans artifacts : dict_keys(['scaler', 'model', 'metadata'])\n",
      "ğŸ” ClÃ©s disponibles dans metadata : dict_keys(['optimal_threshold', 'training_date', 'features', 'dtype_example', 'training_stats', 'performance_metrics', 'training_strategy'])\n",
      "âœ… Nombre de features : 85\n",
      "Extrait des features : ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_CREDIT', 'ANNUITY_INCOME_PERC']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Chargement\n",
    "artifacts = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Inspection des clÃ©s principales\n",
    "print(\"ğŸ” ClÃ©s disponibles dans artifacts :\", artifacts.keys())\n",
    "\n",
    "# Si metadata est bien lÃ , on regarde ce qu'il contient\n",
    "metadata = artifacts.get(\"metadata\", {})\n",
    "print(\"ğŸ” ClÃ©s disponibles dans metadata :\", metadata.keys())\n",
    "\n",
    "# On vÃ©rifie si les features sont bien dÃ©finies\n",
    "features = metadata.get(\"features\", None)\n",
    "if features is None:\n",
    "    print(\"âŒ 'features' est manquant dans metadata.\")\n",
    "else:\n",
    "    print(f\"âœ… Nombre de features : {len(features)}\")\n",
    "    print(\"Extrait des features :\", features[:5])  # Affiche les 5 premiÃ¨res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Type des artifacts : <class 'numpy.ndarray'>\n",
      "ğŸ” Extrait des artifacts : ['EXT_SOURCE_1' 'EXT_SOURCE_2' 'EXT_SOURCE_3' 'AMT_CREDIT'\n",
      " 'ANNUITY_INCOME_PERC']\n",
      "longueur du tableau 85\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Charge le fichier artifact contenant les informations\n",
    "artifact_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Charger les artifacts\n",
    "with open(artifact_path, 'rb') as f:\n",
    "    artifacts = pickle.load(f)\n",
    "\n",
    "# Afficher le type de l'objet chargÃ© pour mieux comprendre sa structure\n",
    "print(\"ğŸ” Type des artifacts :\", type(artifacts))\n",
    "\n",
    "# Afficher un extrait des donnÃ©es pour comprendre la structure\n",
    "print(\"ğŸ” Extrait des artifacts :\", artifacts[:5])  # Affiche les 5 premiers Ã©lÃ©ments si c'est un tableau\n",
    "print(\"longueur du tableau\" ,len(artifacts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Toutes les colonnes nÃ©cessaires sont prÃ©sentes dans les donnÃ©es.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose que client_data est un DataFrame que tu veux prÃ©dire\n",
    "TEST_SAMPLE_FOR_API = \"../backend/data/test_1000_sample_for_api.csv\"\n",
    "TEST_SAMPLE_WITH_TARGET = \"../backend/data/test_1000_sample_with_target.csv\"\n",
    "\n",
    "df_test = pd.read_csv(TEST_SAMPLE_WITH_TARGET)\n",
    "df_api = pd.read_csv(TEST_SAMPLE_FOR_API)\n",
    "\n",
    "client_id = df_test[\"SK_ID_CURR\"]\n",
    "client_data = df_api[df_api[\"SK_ID_CURR\"] == client_id].drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# Comparer les features attendues avec les donnÃ©es d'entrÃ©e\n",
    "expected_features = artifacts\n",
    "\n",
    "# Trouver les colonnes manquantes dans les donnÃ©es d'entrÃ©e\n",
    "missing_cols = set(expected_features) - set(client_data.columns)\n",
    "\n",
    "# Afficher les rÃ©sultats\n",
    "if missing_cols:\n",
    "    print(\"âš ï¸ Colonnes manquantes dans les donnÃ©es d'entrÃ©e :\", missing_cols)\n",
    "else:\n",
    "    print(\"âœ… Toutes les colonnes nÃ©cessaires sont prÃ©sentes dans les donnÃ©es.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProbabilitÃ© de dÃ©faut : 3.91%\n",
      "DÃ©cision : âœ… AcceptÃ©\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Charger les artefacts sauvegardÃ©s\n",
    "model_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "artifacts = joblib.load(model_path)\n",
    "\n",
    "# Extraire le modÃ¨le et le scaler\n",
    "model = artifacts['model']\n",
    "scaler = artifacts['scaler']\n",
    "\n",
    "# Si nÃ©cessaire, obtenir les mÃ©tadonnÃ©es\n",
    "metadata = artifacts['metadata']\n",
    "optimal_threshold = metadata['optimal_threshold']\n",
    "features = metadata['features']\n",
    "\n",
    "# VÃ©rifier les donnÃ©es d'entrÃ©e (client_data)\n",
    "client_data = client_data[features]  \n",
    "\n",
    "# Appliquer le scaler sur les donnÃ©es du client\n",
    "client_data_scaled = scaler.transform(client_data)\n",
    "\n",
    "# Appliquer le scaler en conservant les noms de colonnes\n",
    "client_data_scaled = pd.DataFrame(\n",
    "    scaler.transform(client_data),\n",
    "    columns=client_data.columns,\n",
    "    index=client_data.index\n",
    ")\n",
    "\n",
    "# Faire une prÃ©diction sur les donnÃ©es du client\n",
    "pred_proba = model.predict_proba(client_data_scaled)[:, 1]  # ProbabilitÃ© de dÃ©faut\n",
    "\n",
    "# Appliquer le seuil optimal pour la prÃ©diction finale\n",
    "pred = (pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Affichage des rÃ©sultats\n",
    "print(f\"ProbabilitÃ© de dÃ©faut : {pred_proba[0]:.2%}\")\n",
    "print(f\"DÃ©cision : {'ğŸ›‘ RefusÃ©' if pred[0] == 1 else 'âœ… AcceptÃ©'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Type des artifacts : <class 'numpy.ndarray'>\n",
      "ğŸ” Forme des artifacts : (85,)\n",
      "ğŸ” Extrait des artifacts : ['EXT_SOURCE_1' 'EXT_SOURCE_2' 'EXT_SOURCE_3' 'AMT_CREDIT'\n",
      " 'ANNUITY_INCOME_PERC']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Charge les artifacts depuis le fichier\n",
    "artifact_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Charger les artifacts\n",
    "with open(artifact_path, 'rb') as f:\n",
    "    artifacts = pickle.load(f)\n",
    "\n",
    "# VÃ©rifier la structure des artifacts\n",
    "print(\"ğŸ” Type des artifacts :\", type(artifacts))\n",
    "print(\"ğŸ” Forme des artifacts :\", artifacts.shape)  # VÃ©rifie si c'est un tableau NumPy et sa taille\n",
    "\n",
    "# Afficher un extrait pour mieux comprendre la structure\n",
    "print(\"ğŸ” Extrait des artifacts :\", artifacts[:5])  # Affiche les 5 premiers Ã©lÃ©ments si c'est un tableau\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
