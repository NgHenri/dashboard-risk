{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Clés disponibles dans artifacts : dict_keys(['scaler', 'model', 'metadata'])\n",
      "🔍 Clés disponibles dans metadata : dict_keys(['optimal_threshold', 'training_date', 'features', 'dtype_example', 'training_stats', 'performance_metrics', 'training_strategy'])\n",
      "✅ Nombre de features : 85\n",
      "Extrait des features : ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'AMT_CREDIT', 'ANNUITY_INCOME_PERC']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Chargement\n",
    "artifacts = joblib.load(MODEL_PATH)\n",
    "\n",
    "# Inspection des clés principales\n",
    "print(\"🔍 Clés disponibles dans artifacts :\", artifacts.keys())\n",
    "\n",
    "# Si metadata est bien là, on regarde ce qu'il contient\n",
    "metadata = artifacts.get(\"metadata\", {})\n",
    "print(\"🔍 Clés disponibles dans metadata :\", metadata.keys())\n",
    "\n",
    "# On vérifie si les features sont bien définies\n",
    "features = metadata.get(\"features\", None)\n",
    "if features is None:\n",
    "    print(\"❌ 'features' est manquant dans metadata.\")\n",
    "else:\n",
    "    print(f\"✅ Nombre de features : {len(features)}\")\n",
    "    print(\"Extrait des features :\", features[:5])  # Affiche les 5 premières\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Type des artifacts : <class 'numpy.ndarray'>\n",
      "🔍 Extrait des artifacts : ['EXT_SOURCE_1' 'EXT_SOURCE_2' 'EXT_SOURCE_3' 'AMT_CREDIT'\n",
      " 'ANNUITY_INCOME_PERC']\n",
      "longueur du tableau 85\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Charge le fichier artifact contenant les informations\n",
    "artifact_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Charger les artifacts\n",
    "with open(artifact_path, 'rb') as f:\n",
    "    artifacts = pickle.load(f)\n",
    "\n",
    "# Afficher le type de l'objet chargé pour mieux comprendre sa structure\n",
    "print(\"🔍 Type des artifacts :\", type(artifacts))\n",
    "\n",
    "# Afficher un extrait des données pour comprendre la structure\n",
    "print(\"🔍 Extrait des artifacts :\", artifacts[:5])  # Affiche les 5 premiers éléments si c'est un tableau\n",
    "print(\"longueur du tableau\" ,len(artifacts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toutes les colonnes nécessaires sont présentes dans les données.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Suppose que client_data est un DataFrame que tu veux prédire\n",
    "TEST_SAMPLE_FOR_API = \"../backend/data/test_1000_sample_for_api.csv\"\n",
    "TEST_SAMPLE_WITH_TARGET = \"../backend/data/test_1000_sample_with_target.csv\"\n",
    "\n",
    "df_test = pd.read_csv(TEST_SAMPLE_WITH_TARGET)\n",
    "df_api = pd.read_csv(TEST_SAMPLE_FOR_API)\n",
    "\n",
    "client_id = df_test[\"SK_ID_CURR\"]\n",
    "client_data = df_api[df_api[\"SK_ID_CURR\"] == client_id].drop(columns=[\"SK_ID_CURR\"])\n",
    "\n",
    "# Comparer les features attendues avec les données d'entrée\n",
    "expected_features = artifacts\n",
    "\n",
    "# Trouver les colonnes manquantes dans les données d'entrée\n",
    "missing_cols = set(expected_features) - set(client_data.columns)\n",
    "\n",
    "# Afficher les résultats\n",
    "if missing_cols:\n",
    "    print(\"⚠️ Colonnes manquantes dans les données d'entrée :\", missing_cols)\n",
    "else:\n",
    "    print(\"✅ Toutes les colonnes nécessaires sont présentes dans les données.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilité de défaut : 3.91%\n",
      "Décision : ✅ Accepté\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Charger les artefacts sauvegardés\n",
    "model_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "artifacts = joblib.load(model_path)\n",
    "\n",
    "# Extraire le modèle et le scaler\n",
    "model = artifacts['model']\n",
    "scaler = artifacts['scaler']\n",
    "\n",
    "# Si nécessaire, obtenir les métadonnées\n",
    "metadata = artifacts['metadata']\n",
    "optimal_threshold = metadata['optimal_threshold']\n",
    "features = metadata['features']\n",
    "\n",
    "# Vérifier les données d'entrée (client_data)\n",
    "client_data = client_data[features]  \n",
    "\n",
    "# Appliquer le scaler sur les données du client\n",
    "client_data_scaled = scaler.transform(client_data)\n",
    "\n",
    "# Appliquer le scaler en conservant les noms de colonnes\n",
    "client_data_scaled = pd.DataFrame(\n",
    "    scaler.transform(client_data),\n",
    "    columns=client_data.columns,\n",
    "    index=client_data.index\n",
    ")\n",
    "\n",
    "# Faire une prédiction sur les données du client\n",
    "pred_proba = model.predict_proba(client_data_scaled)[:, 1]  # Probabilité de défaut\n",
    "\n",
    "# Appliquer le seuil optimal pour la prédiction finale\n",
    "pred = (pred_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"Probabilité de défaut : {pred_proba[0]:.2%}\")\n",
    "print(f\"Décision : {'🛑 Refusé' if pred[0] == 1 else '✅ Accepté'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Type des artifacts : <class 'numpy.ndarray'>\n",
      "🔍 Forme des artifacts : (85,)\n",
      "🔍 Extrait des artifacts : ['EXT_SOURCE_1' 'EXT_SOURCE_2' 'EXT_SOURCE_3' 'AMT_CREDIT'\n",
      " 'ANNUITY_INCOME_PERC']\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Charge les artifacts depuis le fichier\n",
    "artifact_path = \"../backend/models/lightgbm_production_artifact_20250415_081218.pkl\"\n",
    "\n",
    "# Charger les artifacts\n",
    "with open(artifact_path, 'rb') as f:\n",
    "    artifacts = pickle.load(f)\n",
    "\n",
    "# Vérifier la structure des artifacts\n",
    "print(\"🔍 Type des artifacts :\", type(artifacts))\n",
    "print(\"🔍 Forme des artifacts :\", artifacts.shape)  # Vérifie si c'est un tableau NumPy et sa taille\n",
    "\n",
    "# Afficher un extrait pour mieux comprendre la structure\n",
    "print(\"🔍 Extrait des artifacts :\", artifacts[:5])  # Affiche les 5 premiers éléments si c'est un tableau\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
