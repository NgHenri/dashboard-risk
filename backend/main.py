# === 0. Imports ===
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Security, Query, Depends, Response, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import APIKeyHeader
from fastapi.responses import JSONResponse
import time
from pydantic import BaseModel
from typing import List, Dict, Optional
from pathlib import Path
import joblib
import numpy as np
import shap
import pandas as pd
import warnings
import logging
import sys
import os
import asyncio
from contextlib import asynccontextmanager
from redis.exceptions import TimeoutError, ConnectionError, RedisError
from redis.backoff import ExponentialBackoff

# Cache et Redis
from fastapi_cache.decorator import cache
from redis.asyncio import Redis
from redis.asyncio.retry import Retry
from fastapi_cache import FastAPICache, coder as _fastapi_coder
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.backends.inmemory import InMemoryBackend
from functools import lru_cache

# from cachetools import TTLCache, cached
from collections.abc import AsyncIterator

import re
from redis import asyncio as aioredis
import json

# === 1. Chargement des variables d'environnement ===
# load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))
env_path = os.path.abspath(os.path.join(os.getcwd(), "..", ".env"))
load_dotenv(dotenv_path=env_path)

# === 2. D√©sactivation des warnings inutiles ===
warnings.filterwarnings("ignore", category=UserWarning)


# === 3. Logger global ===


# ‚Äî filtre global pour redaction ‚Äî
class RedisCredentialFilter(logging.Filter):
    def filter(self, record):
        record.msg = re.sub(
            r"rediss://default:[^@]+@", "rediss://default:****@", str(record.msg)
        )
        return True


logger = logging.getLogger("main")
logger.setLevel(logging.INFO)
logger.addFilter(RedisCredentialFilter())
handler = logging.StreamHandler(sys.stdout)
handler.setFormatter(
    logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
)
logger.addHandler(handler)


async def warmup_redis(client: Redis, count: int = 5):
    async with client.pipeline(transaction=True) as pipe:
        for i in range(count):
            pipe.ping()
        await pipe.execute()
        logger.info(f"‚úÖ Pool Redis pr√©chauff√© avec {count} connexions")


@asynccontextmanager
async def lifespan(app: FastAPI):
    # === Startup ===
    # 1) Initialise Redis
    client = await init_redis_client()
    await warmup_redis(client, count=2)
    app.state.redis_client = client
    if client:
        FastAPICache.init(RedisBackend(client), prefix="demo-cache")
        logger.info("Cache logging activ√©")
    else:
        FastAPICache.init(InMemoryBackend(), prefix="memory-only")
        logger.warning("üõë Redis non disponible, utilisant un cache en m√©moire.")

    # 2) G√©n√®re et injecte l'exemple al√©atoire pour /predict
    try:
        example_id, example_data = random_client_example()  # renvoie juste le dict data
        ClientData.model_config["json_schema_extra"]["example"] = {"data": example_data}
        logger.info("‚úÖ Exemple dynamique inject√© dans ClientData")
        app.state.example_id = example_id

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible d'injecter l'exemple dynamique : {e}")

    yield  # Fin du startup, d√©but du runtime
    # === Shutdown ===
    if client:
        try:
            await client.close()
            logger.info("üßπ Connexion Redis ferm√©e proprement")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur √† la fermeture Redis : {e}")
    else:
        logger.info("‚ÑπÔ∏è Aucun client Redis √† fermer")


# === 4. FastAPI app ===
app = FastAPI(lifespan=lifespan)

# === 5. Middleware CORS ===
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*", "x-api-key"],
)

# === 6. Cl√© API ===
api_key_header = APIKeyHeader(name="x-api-key")
API_KEY = os.getenv("API_KEY")
API_URL = os.getenv("API_URL")

# === 7. global ===
LATENCY_THRESHOLD_MS = 200  # seuil acceptable
REDIS_MEMORY_WARNING = 200  # Mo


# === 8. Validation de la cl√© API ===
async def validate_api_key(api_key: str = Security(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Cl√© API invalide")
    return api_key


# === 9. Initialisation dynamique de Redis ===


async def init_redis_client() -> Redis | None:
    logger.info("üß™ Appel de init_redis_client()")
    try:
        upstash_url = os.getenv("UPSTASH_REDIS_URL", "")
        upstash_token = os.getenv("UPSTASH_REDIS_TOKEN", "")

        if upstash_url and upstash_token:
            # Nettoyer et normaliser le host:port
            host = upstash_url.replace("https://", "").replace("http://", "")
            if ":" not in host:
                host = f"{host}:6379"

            redis_url = f"rediss://default:{upstash_token}@{host}"
            logger.info(f"Upstash host: {host}")  # üëà safe
            logger.info(f"‚Üí Upstash URI: {redis_url}")  # üîí sera automatiquement filtr√©

            client = Redis.from_url(
                redis_url,
                decode_responses=False,
                max_connections=20,
                health_check_interval=30,
                retry=Retry(ExponentialBackoff(), 3),
                retry_on_timeout=True,
                socket_connect_timeout=3,  # Timeout de connexion r√©duit
                socket_timeout=10,
                socket_keepalive=True,
            )
            logger.info("üîå Connexion √† Redis via Upstash")
        else:
            client = Redis.from_url("redis://localhost:6379", decode_responses=False)
            logger.info("üñ•Ô∏è Connexion √† Redis local")

        pong = await client.ping()
        if pong is not True:
            raise ConnectionError("R√©ponse Redis invalide")

        logger.info("‚úÖ Redis configur√© avec succ√®s")
        return client

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Cache m√©moire activ√© - Erreur Redis : {e}")
        FastAPICache.init(InMemoryBackend())
        return None


# === 10 Configuration Redis et Cache ===


async def get_healthy_client(request: Request) -> Redis | None:
    client = request.app.state.redis_client
    if not client:
        return None

    try:
        await client.ping()
        return client
    except (TimeoutError, ConnectionError, RedisError) as e:
        logger.warning(f"Redis error: {e} ‚Üí Reconnexion...")
        try:
            await client.aclose()  # Fermeture asynchrone
        except Exception as e:
            logger.error(f"Fermeture client √©chou√©e: {e}")

        try:
            new_client = await init_redis_client()  # Nouvelle tentative
            request.app.state.redis_client = new_client
            return new_client
        except Exception as e:
            logger.critical(f"√âchec reconnexion Redis: {e}")
            return None


# === 11. Logger de cache (optionnel) ===
async def get_redis(request: Request):
    client = request.app.state.redis_client
    if not client:
        raise HTTPException(503, "Redis non initialis√©")
    return client


# === 12. LoggingRedisBackend ===
class LoggingRedisBackend(RedisBackend):
    async def set(self, key: str, value: str, expire: int):
        logger.info(
            f"SET CACHE: {key} (TTL: {expire}s)"
        )  # Log lors de la mise en cache
        return await super().set(key, value, expire)


# === 13. Script de rotation (√† ex√©cuter p√©riodiquement)  ===

import requests


def rotate_redis_token():
    headers = {"Authorization": f"Bearer {CURRENT_TOKEN}"}
    response = requests.post(
        "https://api.upstash.com/v2/tokens/rotate", headers=headers
    )
    new_token = response.json()["new_token"]
    update_env_file("UPSTASH_REDIS_TOKEN", new_token)


BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# === 14. Chargement des artefacts ===
# ARTIFACT_PATH = "models/lightgbm_production_artifact_20250415_081218.pkl"
ARTIFACT_PATH = os.path.join(
    BASE_DIR, "models", "lightgbm_production_artifact_20250415_081218.pkl"
)

try:
    artifacts = joblib.load(ARTIFACT_PATH)
    model = artifacts["model"]
    scaler = artifacts["scaler"]
    features = artifacts["metadata"]["features"]
    threshold = artifacts["metadata"]["optimal_threshold"]
    logger.info("Artefacts charg√©s avec succ√®s.")
except Exception as e:
    logger.critical(f"Erreur de chargement des artefacts : {e}")
    raise RuntimeError("Impossible de charger les artefacts du mod√®le.")

# === 15. Initialisation SHAP ===
explainer = shap.Explainer(model)
logger.info(f"Type de expected_value: {type(explainer.expected_value)}")
logger.info(f"Valeur de expected_value: {explainer.expected_value}")

# === 16. Chargement des donn√©es globales ===
# GLOBAL_DATA_PATH = "data/test_2000_sample_for_api.csv"
GLOBAL_DATA_PATH = os.path.join(BASE_DIR, "data", "test_2000_sample_for_api.csv")

try:
    df_global = pd.read_csv(GLOBAL_DATA_PATH)[features]
    df_global_scaled = scaler.transform(df_global)  # Doit √™tre un array numpy 2D
    assert df_global_scaled.shape[1] == len(features), "Incoh√©rence features/scaler !"
    logger.info("Donn√©es globales charg√©es et pr√©trait√©es.")
except Exception as e:
    logger.critical(f"Erreur de chargement ou traitement des donn√©es globales : {e}")
    raise RuntimeError("√âchec de la pr√©paration des donn√©es globales.")

# === 17. Calcul SHAP global d√®s le d√©marrage ===
try:
    global_shap_values = explainer.shap_values(df_global_scaled)
    if isinstance(global_shap_values, list):  # Cas classification binaire
        global_shap_values = global_shap_values[
            1
        ]  # Garder seulement la classe positive

    # === V√âRIFICATION PRIMAIRE ===
    assert (
        len(df_global) == global_shap_values.shape[0]
    ), f"Donn√©es/SHAP incoh√©rents ({len(df_global)} vs {global_shap_values.shape[0]})"

    global_shap_mean = global_shap_values.mean(axis=0)
except Exception as e:
    logger.critical(f"Erreur calcul SHAP global : {str(e)}")
    raise RuntimeError("Impossible de calculer les SHAP globaux")

# === V√âRIFICATION REDONDANTE POUR S√âCURIT√â ===
global_shap_matrix = global_shap_values
assert len(df_global) == len(
    global_shap_matrix
), f"Donn√©es/SHAP incoh√©rents ({len(df_global)} vs {len(global_shap_matrix)})"


# === 18. Liste des clients ===
try:
    full_df = pd.read_csv(GLOBAL_DATA_PATH)
    client_ids = full_df["SK_ID_CURR"].dropna().astype(int).unique().tolist()
    logger.info(f"Chargement de {len(client_ids)} clients r√©ussis")
except Exception as e:
    logger.critical(f"√âchec du chargement des donn√©es : {str(e)}")
    raise RuntimeError("Impossible de d√©marrer l'API - donn√©es corrompues")


# ==== test ==== ===============================================================


@app.get("/", include_in_schema=False)
async def read_root():
    return {"message": "Bienvenue sur l'API !"}


@app.get("/cache-example", include_in_schema=False)
@cache(expire=30)
async def cache_example():
    logger.info("Cache utilis√©")  # Log lors de l'appel de la fonction
    return {"message": "Ce message est mis en cache pendant 60 secondes"}


@app.get("/test-redis", include_in_schema=False)
async def test_redis():
    redis_client = await app.state.redis_client
    if redis_client is None:
        return {"status": "‚ùå Redis non initialis√©"}
    try:
        pong = await redis_client.ping()
        return {"status": f"‚úÖ Redis OK: {pong}"}
    except Exception as e:
        return {"status": f"‚ùå Erreur Redis: {str(e)}"}


# ========== all data ===============================


@app.get("/get_test_data", include_in_schema=False)
@cache(expire=3600)
async def get_test_data(_: str = Depends(validate_api_key)):
    """Renvoie les donn√©es de test"""
    try:
        if full_df.empty:
            raise HTTPException(status_code=404, detail="Aucune donn√©e disponible")

        return full_df.to_dict(orient="records")

    except Exception as e:
        logger.error(f"Erreur get_test_data : {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Erreur de chargement des donn√©es")


# ============ IDS Client ================================


@app.get("/client_ids", include_in_schema=False)
def get_client_ids(limit: int = 2000):
    """Renvoie la liste des IDs clients"""
    try:
        if not client_ids:
            logger.warning("Aucun client trouv√© dans les donn√©es")
            raise HTTPException(status_code=404, detail="Aucun client disponible")

        return {"client_ids": client_ids[:limit]}

    except Exception as e:
        logger.error(f"Erreur /client_ids : {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Erreur interne du serveur")


# ====== basemodel =======================================


class ClientData(BaseModel):
    data: dict

    model_config = {
        "json_schema_extra": {"example": {}}  # on va remplacer √ßa au startup
    }


# fonction ‚Äúrandom‚Äù (pas de route, juste un utilitaire)
def random_client_example() -> dict:
    if full_df.empty:
        raise RuntimeError("full_df vide")

    row = full_df.sample(1).iloc[0].to_dict()
    client_id = row.pop("SK_ID_CURR", None)
    if client_id is None:
        raise RuntimeError("SK_ID_CURR manquant")

    # On retourne juste le champ `data` car on ne veut pr√©remplir que `data`
    example_id = int(client_id)
    return example_id, row


# ============= info Client =============================


@app.get("/client_info/{client_id}", response_model=dict)
def get_client_info(client_id: int):
    """
    Retourne les informations d√©taill√©es d'un client
    - **client_id** : Identifiant unique du client (ex: 100001)
    """
    try:
        client_data = full_df.loc[full_df["SK_ID_CURR"] == client_id].copy()

        if client_data.empty:
            raise HTTPException(status_code=404, detail="Client introuvable")

        return client_data.iloc[0].to_dict()

    except Exception as e:
        logger.error(f"Erreur client {client_id} : {str(e)}")
        raise HTTPException(
            status_code=500, detail="Erreur technique - contactez l'administrateur"
        )


@app.get("/client_shap_data/{client_id}", include_in_schema=False)
def get_client_shap_data(client_id: int):
    # 1) on r√©utilise get_client_info pour lever 404 si besoin
    info = get_client_info(client_id)
    # 2) on filtre seulement les colonnes du mod√®le
    try:
        return {feat: info[feat] for feat in features}
    except KeyError as e:
        # au cas o√π un feature manquerait (devrait rarement arriver)
        raise HTTPException(
            status_code=500, detail=f"Probl√®me d‚Äôacc√®s √† la feature {e.args[0]}"
        )


# === stat de polulation =====================


@app.post("/population_stats")
def get_population_stats(feature_request: dict):
    """
    Calcule les statistiques de population pour une feature donn√©e
    Format attendu :
    {
    "feature": "AMT_CREDIT",
    "filters": {
        "CODE_GENDER": 0,
        "FLAG_OWN_CAR": 1
    },
    "sample_size": 1000
    }

    - Combinaison de filtres (femmes propri√©taires de voiture)

    - Taille d'√©chantillon max
    """
    try:
        # Validation des entr√©es
        feature = feature_request.get("feature")
        if feature not in full_df.columns:
            raise HTTPException(status_code=400, detail="Feature invalide")

        # Application des filtres
        filtered_df = full_df.copy()
        for f, v in feature_request.get("filters", {}).items():
            if f in filtered_df.columns:
                filtered_df = filtered_df[filtered_df[f] == v]

        # √âchantillonnage
        sample_size = min(feature_request.get("sample_size", 1000), len(filtered_df))
        sample_df = (
            filtered_df.sample(sample_size, random_state=42)
            if not filtered_df.empty
            else pd.DataFrame()
        )

        # Calcul des statistiques
        stats = {
            "mean": float(sample_df[feature].mean()),
            "std": float(sample_df[feature].std()),
            "min": float(sample_df[feature].min()),
            "25%": float(sample_df[feature].quantile(0.25)),
            "50%": float(sample_df[feature].quantile(0.5)),
            "75%": float(sample_df[feature].quantile(0.75)),
            "max": float(sample_df[feature].max()),
        }

        return {"feature": feature, "stats": stats}

    except Exception as e:
        logger.error(f"Erreur population_stats : {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


# ====== probabilit√© =====


@app.post("/predict")
def predict(client: ClientData):
    """
    Pr√©dit la probabilit√© de d√©faut d'un client et donne une d√©cision (Accept√©/Refus√©).

    Les donn√©es client sont transform√©es, scal√©es et pass√©es au mod√®le pour obtenir la probabilit√©.
    Une d√©cision est ensuite prise selon un seuil optimis√© (threshold).

    Args:
        client (ClientData): dictionnaire contenant les valeurs des features du client.

    Returns:
        dict: {
            "probability": probabilit√© de d√©faut (en pourcentage),
            "decision": "‚úÖ Accept√©" ou "‚ùå Refus√©"
        }
    """
    try:
        # Transformer les donn√©es d'entr√©e en DataFrame et appliquer le scaler
        X = pd.DataFrame([client.data])[features]
        X_scaled = scaler.transform(X)

        # Calculer la probabilit√© de d√©faut
        prob = model.predict_proba(X_scaled)[0, 1]

        # D√©finir la d√©cision selon le seuil
        decision = "‚úÖ Accept√©" if prob < threshold else "‚ùå Refus√©"

        return {"probability": round(prob * 100, 2), "decision": decision}
    except Exception as e:
        raise HTTPException(
            status_code=400, detail=f"Erreur dans la pr√©diction : {str(e)}"
        )


# ==== batch de clients ====


class BatchPredictionRequest(BaseModel):
    data: List[dict]


@app.post("/predict_batch", include_in_schema=False)
def predict_batch(batch: BatchPredictionRequest):
    """
    Traite un lot de donn√©es clients et renvoie les probabilit√©s de d√©faut
    ainsi que la d√©cision pour chaque client.

    Cette route re√ßoit en entr√©e une liste de dictionnaires repr√©sentant
    plusieurs clients (`batch.data`), applique le scaler et le mod√®le pour
    pr√©dire la probabilit√© de d√©faut, et formate une d√©cision binaire
    selon le seuil optimis√©.

    Args:
        batch (BatchPredictionRequest): Objet Pydantic contenant :
            - data (List[Dict]): liste des enregistrements clients bruts,
            chacun devant contenir la cl√© "SK_ID_CURR" et les features
            n√©cessaires au mod√®le.

    Returns:
        List[Dict]: Pour chaque client fourni :
            - "id" (int | None): l‚Äôidentifiant client (ou None s‚Äôil est absent)
            - "probability" (float): probabilit√© de d√©faut en pourcentage
            - "decision" (str): "‚úÖ Accept√©" si probabilit√© < threshold,
            sinon "‚ùå Refus√©"

    Raises:
        HTTPException(400): Si une erreur se produit pendant la transformation
                            ou la pr√©diction (format des donn√©es, dimensions, etc.).
    """
    try:
        # Transformer en DataFrame
        df = pd.DataFrame(batch.data)

        # S√©lection des colonnes dans le bon ordre
        X = df[features]

        # Scaling
        X_scaled = scaler.transform(X)

        # Pr√©dictions
        probs = model.predict_proba(X_scaled)[:, 1]
        decisions = ["‚úÖ Accept√©" if p < threshold else "‚ùå Refus√©" for p in probs]

        results = [
            {
                "id": row.get("SK_ID_CURR", None),
                "probability": round(p * 100, 2),
                "decision": d,
            }
            for row, p, d in zip(batch.data, probs, decisions)
        ]

        return results
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur batch : {str(e)}")


# ======= √©chantillon de donn√©es ==============


@app.get("/global_shap_sample", include_in_schema=False)
@cache(expire=3600)
async def get_global_shap_sample(sample_size: int = 1000):
    """
    R√©cup√®re un √©chantillon des donn√©es globales non-scal√©es.

    Cette route renvoie un sous-ensemble al√©atoire des enregistrements
    du jeu de donn√©es global (avant mise √† l‚Äô√©chelle), destin√© √† √™tre
    utilis√© c√¥t√© frontend pour g√©n√©rer des plots ou faire des calculs.

    Args:
        sample_size (int, optional): Nombre maximal de lignes √† retourner.
                                    Si la population contient moins de lignes,
                                    on renvoie tout le DataFrame. (par d√©faut 1000)

    Returns:
        List[dict]: Liste de dictionnaires, un par enregistrement,
                    conforme √† `df_global.to_dict(orient="records")`.

    Raises:
        HTTPException(500): En cas d‚Äôerreur lors de l‚Äô√©chantillonnage.
    """
    try:
        sample = df_global.sample(min(sample_size, len(df_global)), random_state=42)
        return sample.to_dict(orient="records")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ========== les valeurs SHAP pour un jeu de donn√©es client avec ID


@app.get("/shap/local/{client_id}")
async def get_local_shap(client_id: int):
    """
    Calcule et renvoie les valeurs SHAP pour un client existant.

    Args:
        client_id (int): Identifiant du client.

    Returns:
        dict: {
            "values": List[float],        # Valeurs SHAP pour chaque feature
            "base_value": float,         # Valeur de base (expected_value)
            "features": List[str],       # Liste des noms de features
            "feature_names": List[str],  # (m√™me que `features`)
            "client_data": dict          # Donn√©es brutes du client
        }

    Raises:
        HTTPException(404): Si l‚ÄôID n‚Äôexiste pas.
        HTTPException(500): En cas d‚Äôerreur interne.
    """
    try:
        # R√©cup√©ration des donn√©es client
        client_data = full_df[full_df["SK_ID_CURR"] == client_id][features]
        if client_data.empty:
            raise HTTPException(status_code=404, detail="Client introuvable")

        # Pr√©traitement des donn√©es
        X_scaled = scaler.transform(client_data.values.reshape(1, -1))

        # Calcul SHAP
        shap_values = explainer.shap_values(X_scaled)
        base_value = (
            explainer.expected_value[1]
            if isinstance(explainer.expected_value, list)
            else explainer.expected_value
        )

        return {
            "values": shap_values[0].tolist(),
            "base_value": float(base_value),
            "features": features,
            "client_data": client_data.iloc[0].to_dict(),
            "feature_names": list(features),
        }

    except Exception as e:
        logger.error(f"Erreur SHAP locale : {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


# ==========les valeurs SHAP pour un jeu de donn√©es client arbitraire


@app.post("/shap/local")
def get_local_shap_by_payload(client: ClientData):
    """
    Calcule les valeurs SHAP pour un jeu de donn√©es client arbitraire
    et renvoie un r√©sum√© des 10 features les plus influentes.

    Args:
        client (ClientData): Pydantic model contenant `"data": {feature: valeur,‚Ä¶}`.

    Returns:
        dict: {
            "base_value": float,      # Valeur de base (expected_value)
            "values": List[float],    # Valeurs SHAP brutes
            "features": List[str],    # Liste des noms de features
            "explanation": List[{     # Top-10 features class√©es par contribution
                "feature": str,
                "shap_value": float
            }]
        }

    Raises:
        HTTPException(400): Si des features manquent dans l‚Äôentr√©e.
        HTTPException(500): En cas d‚Äôerreur interne.
    """
    try:
        # Validation des donn√©es d'entr√©e
        missing_features = [feat for feat in features if feat not in client.data]
        if missing_features:
            raise ValueError(f"Features manquantes : {missing_features}")

        # Transformation des donn√©es
        X = pd.DataFrame([client.data])[features]
        X_scaled = scaler.transform(X)

        # Calcul SHAP avec gestion multi-classe
        shap_values = explainer.shap_values(X_scaled)
        if isinstance(explainer.expected_value, list):
            base_value = float(explainer.expected_value[1])  # Classification binaire
        else:
            base_value = float(explainer.expected_value)  # R√©gression

        return {
            "base_value": base_value,
            "values": shap_values[0].tolist(),
            "features": features,
            "explanation": [
                {"feature": f, "shap_value": round(float(v), 4)}
                for f, v in sorted(
                    zip(features, shap_values[0]), key=lambda x: abs(x[1]), reverse=True
                )[:10]
            ],
        }

    except Exception as e:
        logger.error(f"Erreur SHAP locale : {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Erreur de calcul SHAP")

    except ValueError as e:
        logger.warning(f"Donn√©es invalides : {str(e)}")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        logger.error(f"Erreur SHAP locale : {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail="Erreur de calcul SHAP")


# ====================
@cache(expire=3600)  # Cache 1 heure
@app.get("/global_features", include_in_schema=False)
def get_global_features(top_n: int = 10):
    """Renvoie les features ayant le plus grand impact moyen (directionnel) sur les pr√©dictions du mod√®le.

    Le score moyen est calcul√© √† partir des valeurs SHAP sur l'ensemble global des donn√©es.
    Chaque feature est associ√©e √† :
    - son impact moyen (positif ou n√©gatif)
    - sa direction d'influence (positive = favorise l'acceptation, n√©gative = favorise le refus)

    Args:
        top_n (int): nombre de features √† retourner (class√©es par importance d√©croissante)

    Returns:
        dict: {
            "global_importances": [
                {
                    "feature": nom de la feature,
                    "impact": valeur moyenne (sign√©e) de SHAP,
                    "direction": "positive" ou "negative"
                },
                ...
            ]
        }"""
    try:
        features_sorted = sorted(
            zip(features, global_shap_mean), key=lambda x: abs(x[1]), reverse=True
        )[:top_n]

        return {
            "global_importances": [
                {
                    "feature": feat,
                    "impact": round(float(impact), 4),
                    "direction": "positive" if impact > 0 else "negative",
                }
                for feat, impact in features_sorted
            ]
        }
    except Exception as e:
        logger.error(f"Erreur /global_features : {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Erreur interne lors du calcul des SHAP globaux : {str(e)}",
        )


# ============Au d√©marrage ==================
global_shap_matrix = global_shap_values  # D√©j√† calcul√©


@app.get("/global_shap_matrix", include_in_schema=False)
@cache(expire=3600)
def get_global_shap_matrix(
    sample_size: int = 1000,
    random_state: int = 42,
    long_format: Optional[bool] = Query(
        False, description="Retourne un format long si True"
    ),
):
    """
    Renvoie un √©chantillon des valeurs SHAP globales et des donn√©es associ√©es.

    Cette route permet d‚Äôextraire, pour visualisation (summary_plot, bar chart, beeswarm, etc.),
    un sous-ensemble al√©atoire des contributions SHAP calcul√©es sur l‚Äôensemble global des donn√©es.

    Args:
        sample_size (int, optional): Nombre maximal de lignes √† renvoyer (par d√©faut 1000).
        random_state (int, optional): Graine pour la reproductibilit√© de l‚Äô√©chantillonnage.
        long_format (bool, optional):
            - False (par d√©faut) : format ‚Äúlarge‚Äù renvoyant un JSON
            contenant :
                - shap_values: List[List[float]]
                - feature_values: List[dict]
                - features: List[str]
                - base_value: float
            - True : format ‚Äúlong‚Äù renvoy√© sous forme de liste d‚Äôenregistrements
            (un dict par valeur SHAP), pour faciliter certains traitements.

    Returns:
        dict | List[dict]:
        - Si `long_format=False` : un dict regroupant :
            ‚Ä¢ "shap_values",
            ‚Ä¢ "feature_values",
            ‚Ä¢ "features",
            ‚Ä¢ "base_value".
        - Si `long_format=True` : une liste de dicts (format long).

    Raises:
        HTTPException(500): En cas d‚Äôerreur interne lors de l‚Äô√©chantillonnage ou de la mise en forme.
    """
    try:
        sample_size = min(sample_size, len(global_shap_matrix))
        rng = np.random.default_rng(seed=random_state)
        sample_idx = rng.choice(len(global_shap_matrix), sample_size, replace=False)

        shap_sample = global_shap_matrix[sample_idx]
        feature_sample = df_global.iloc[sample_idx]

        if long_format:
            df_long = get_shap_long_dataframe(shap_sample, feature_sample[features])
            return df_long.to_dict(orient="records")

        return {
            "shap_values": shap_sample.tolist(),
            "feature_values": feature_sample.to_dict(orient="records"),
            "features": features,
            "base_value": float(
                explainer.expected_value[1]
                if isinstance(explainer.expected_value, list)
                else explainer.expected_value
            ),
        }
    except Exception as e:
        logger.error(f"Erreur globale_shap_matrix : {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500, detail="Erreur de traitement des donn√©es globales"
        )


# ==== Monkey-patche ===================================================
from fastapi.openapi.utils import get_openapi


def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    # g√©n√®re le sch√©ma de base
    schema = get_openapi(
        title=app.title,
        version=app.version,
        routes=app.routes,
    )
    # r√©cup√®re l‚ÄôID d‚Äôexemple stock√©
    example_id = getattr(app.state, "example_id", None)
    if example_id is not None:
        # liste des routes √† mettre √† jour
        for route in [
            "/client_info/{client_id}",
            "/shap/local/{client_id}",
            "/client_shap_data/{client_id}",
        ]:
            # r√©cup√®re le bloc "get" pour cette route
            if route in schema["paths"]:
                op = schema["paths"][route].get("get")
                if op and "parameters" in op:
                    for param in op["parameters"]:
                        if param.get("name") == "client_id":
                            param["example"] = example_id
                            break
    app.openapi_schema = schema
    return schema


app.openapi = custom_openapi

# === health ==========
#
#       control et memory
#
# ===== control =======


# ===== control =======
# un cache TTL en m√©moire : max 1 entr√©e, expire au bout de 30 s
# En haut de ton module
async def _raw_redis_memory_usage(redis_client: Redis) -> float:
    try:
        info = await redis_client.info("memory")
        used = info.get("used_memory", 0)

        # Gestion unifi√©e des types
        if isinstance(used, bytes):
            used = used.decode("utf-8")
        used = int(used)

        return used / 1e6
    except Exception as e:
        logger.error(f"Error in memory check: {str(e)}")
        return -1.0


@cache(expire=30)
async def get_redis_memory_usage(redis_client: Redis) -> float:
    """Retourne l'utilisation m√©moire en MB (ou -1.0 si erreur)."""
    return await _raw_redis_memory_usage(redis_client)


# ===== health check endpoint =====


@app.get("/health", include_in_schema=False)
async def health_check(
    redis_client=Depends(get_redis), response: Response = Response()
):
    """Endpoint de sant√© complet avec monitoring Redis"""
    response.headers["Cache-Control"] = "no-store, max-age=0"
    checks = {
        "status": "API op√©rationnelle üöÄ",
        "redis": {
            "status": "inactive",
            "cache_type": "redis" if redis_client else "memory",
            "memory_used": "N/A",
            "latency_ms": None,
        },
        "model": {
            "loaded": model is not None,
            "type": type(model).__name__ if model else None,
        },
        "scaler": {
            "loaded": scaler is not None,
            "type": type(scaler).__name__ if scaler else None,
        },
        "features": {
            "loaded": isinstance(features, list) and len(features) > 0,
            "count": len(features) if features else 0,
        },
        "threshold": threshold if isinstance(threshold, float) else None,
        "shap": {
            "values_ready": isinstance(global_shap_matrix, np.ndarray),
            "mean_ready": isinstance(global_shap_mean, np.ndarray),
        },
        "data": {
            "global_ready": isinstance(df_global, pd.DataFrame) and not df_global.empty,
            "clients_loaded": len(client_ids),
            "sample_size": (
                df_global.shape[0] if isinstance(df_global, pd.DataFrame) else 0
            ),
        },
    }

    # V√©rification Redis avec ping et latence
    if redis_client:
        try:
            t0 = time.perf_counter()
            pong = await redis_client.ping()
            latency = (time.perf_counter() - t0) * 1000
            latency_ms = round(latency, 2)

            # D√©terminer le statut
            if pong and latency_ms < LATENCY_THRESHOLD_MS:
                checks["redis"]["status"] = "active"
            elif pong:
                checks["redis"]["status"] = "unstable"
                logger.warning(f"‚ö†Ô∏è Latence Redis √©lev√©e : {latency_ms:.2f} ms")
            else:
                checks["redis"]["status"] = "unstable"
                logger.warning("‚ö†Ô∏è Redis a r√©pondu mais sans 'pong' explicite")

            checks["redis"]["latency_ms"] = latency_ms
            # checks["redis"]["memory_used"] = await get_redis_memory_usage(redis_client)
            used_mb = await get_redis_memory_usage(redis_client)
            checks["redis"]["memory_used"] = (
                f"{used_mb:.2f} MB" if used_mb >= 0 else "N/A"
            )

        except Exception as e:
            checks["redis"]["status"] = f"error: {str(e)}"
            logger.error(f"‚ùå Erreur Redis : {str(e)}")

    # D√©termination du statut global
    critical_services = [
        checks["model"]["loaded"],
        checks["scaler"]["loaded"],
        checks["features"]["loaded"],
        checks["data"]["global_ready"],
        checks["redis"]["status"] in ["active", "unstable"],  # Redis non critique
    ]

    if not all(critical_services):
        checks["status"] = "üî¥ API non op√©rationnelle"
    elif not all(
        [
            checks["shap"]["values_ready"],
            checks["shap"]["mean_ready"],
            checks["data"]["clients_loaded"] > 0,
        ]
    ):
        checks["status"] = "üü° API partiellement op√©rationnelle"

    code = 200 if checks["redis"]["status"] in ("active", "unstable") else 503
    return JSONResponse(checks, status_code=code)
